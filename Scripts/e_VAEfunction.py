# -*- coding: utf-8 -*-
"""VAE_Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ddgtdvykBjGVu3BuaO9OZ4o2EMp5Vr2l
"""



#!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git#egg=polara

"""##Import"""

from typing import List
import torch
import torch.nn.functional as F
import scipy.sparse as sparse
from torch import nn
import numpy as np
import pandas as pd
from tqdm import trange,tqdm
import sys
from polara.evaluation.pipelines import random_grid
import scipy
import bottleneck as bn
from scipy.sparse import coo_matrix
import scipy.stats as st
from scipy.sparse import csr_matrix, find



"""# Functions

##Data Preparation
"""

def split_users(raw_data,user_col,n_heldout_users):
    unique_uid = raw_data[user_col].unique()
    n_users = unique_uid.size
    np.random.seed(98765)
    idx_perm = np.random.permutation(n_users)
    unique_uid = unique_uid[idx_perm]
    train_users = unique_uid[:(n_users - n_heldout_users * 2)]
    valid_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]
    test_users = unique_uid[(n_users - n_heldout_users):]
    return train_users, valid_users, test_users

def split_train_test(data, test_prop=0.2):
    data_grouped_by_user = data.groupby('userId')
    tr_list, te_list = list(), list()

    np.random.seed(98765)
    for i, (_, group) in enumerate(data_grouped_by_user):
        n_items_u = len(group)
        if n_items_u >= 2:
            idx = np.zeros(n_items_u, dtype='bool')
            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True
            tr_list.append(group[np.logical_not(idx)])
            te_list.append(group[idx])
        else:
            tr_list.append(group)

        if i % 250 == 0:
            print("%d users sampled" % i)
            sys.stdout.flush()

    data_tr = pd.concat(tr_list)
    data_te = pd.concat(te_list)    
    return data_tr, data_te

def numerize_data(data,user_dict,item_dict,user_col,item_col):
    user = [user_dict[x] for x in data[user_col]]
    item = [item_dict[x] for x in data[item_col]]
    df_output =  pd.DataFrame(data={user_col: user, item_col: item}, columns=[user_col,item_col])
    return df_output 

def get_TrainRatMat(DF,user_col,item_col,n_items):   #load_train_data(csv_file)
    n_users = DF[user_col].max() + 1
    rows, cols = DF[user_col], DF[item_col]
    data = sparse.csr_matrix((np.ones_like(rows),(rows, cols)), dtype='float64',
                             shape=(n_users, n_items))
    return data

def getTrainTest_RatMat(df_train, df_test,user_col,item_col,n_items):  # load_tr_te_data(csv_file_tr, csv_file_te):
    start_idx = min(df_train[user_col].min(),df_test[user_col].min())
    end_idx = max(df_train[user_col].max(),df_test[user_col].max())

    rows_train, cols_train = df_train[user_col] - start_idx, df_train[item_col]
    rows_test, cols_test =  df_test[user_col] - start_idx, df_test[item_col]

    data_train = sparse.csr_matrix((np.ones_like(rows_train),
                             (rows_train, cols_train)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))
    data_test = sparse.csr_matrix((np.ones_like(rows_test),
                             (rows_test, cols_test)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))
    return data_train, data_test



"""## Model"""

class Encoder(nn.Module):
    def __init__(self, q_dims, dropout):  ## q_dims: List[int], dropout: List[float]
        super().__init__()

        self.q_dims = q_dims
        q_dims_ = self.q_dims[:-1] + [self.q_dims[-1] * 2]
        self.q_layers = nn.Sequential()
        for i, (p, inp, out) in enumerate(zip(dropout, q_dims_[:-1], q_dims_[1:])):
            self.q_layers.add_module("_".join(["dropout", str(i)]), nn.Dropout(p))
            self.q_layers.add_module("_".join(["linear", str(i)]), nn.Linear(inp, out))

    def forward(self, X):
        h = F.normalize(X, p=2, dim=1)
        for i, layer in enumerate(self.q_layers):
            h = layer(h)
            if i != len(self.q_layers) - 1:
                h = torch.tanh(h)
            else:
                mu, logvar = torch.split(h, self.q_dims[-1], dim=1)
        return mu, logvar

class Decoder(nn.Module):
    def __init__(self, p_dims, dropout):  #p_dims: List[int], dropout: List[float]
        super().__init__()

        self.p_layers = nn.Sequential()
        for i, (p, inp, out) in enumerate(zip(dropout, p_dims[:-1], p_dims[1:])):
            self.p_layers.add_module("_".join(["dropout", str(i)]), nn.Dropout(p))
            self.p_layers.add_module("_".join(["linear", str(i)]), nn.Linear(inp, out))

    def forward(self, X):
        h = X
        for i, layer in enumerate(self.p_layers):
            h = layer(h)
            if i != len(self.p_layers) - 1:
                h = torch.tanh(h)
        return h


class MultiVAE(nn.Module):
    def __init__(self,p_dims,q_dims,dropout_enc,dropout_dec): #p_dims:List[int],q_dims:List[int],dropout_enc:List[float],dropout_dec:List[float], 
        super().__init__()
        self.encode = Encoder(q_dims, dropout_enc)  #q_dims == Encoder dimensions
        self.decode = Decoder(p_dims, dropout_dec)  #p_dims == Decoder dimensions

    def forward(self, X):
        mu, logvar = self.encode(X)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        sampled_z = mu + float(self.training) * eps * std
        return self.decode(sampled_z), mu, logvar



"""## Loss & Metric"""

def vae_loss_fn(inp, out, mu, logvar, anneal):
    neg_ll = -torch.mean(torch.sum(F.log_softmax(out, 1) * inp, -1))
    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))
    return neg_ll + anneal * KLD

def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):
    batch_users = X_pred.shape[0]
    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)
    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk_part[:, :k]]
    idx_part = np.argsort(-topk_part, axis=1)
    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]

    # build the discount template
    tp = 1.0 / np.log2(np.arange(2, k + 2))
    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis], idx_topk].toarray() * tp).sum(axis=1)
    IDCG = np.array([(tp[: min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])
    return DCG[IDCG > 0.0] / IDCG[IDCG > 0.0]

#Recall = (Relevant_Items_Recommended in top-k)/(Relevant_Items)

"""## Trad & Valid"""

def train_step_2(model, optimizer, data,total_anneal_steps,epoch):
    model.train()
    running_loss = 0.0
    global update_count
    N = data.shape[0]
    idxlist = list(range(N))
    np.random.shuffle(idxlist)
    t = len(range(0, N, batch_size))   #training_steps
    for batch_idx, start_idx in zip(range(t), range(0, N, batch_size)):
        end_idx = min(start_idx + batch_size, N)
        X_inp = data[idxlist[start_idx:end_idx]]
        X_inp = torch.FloatTensor(X_inp.toarray()).to(device)
        if constant_anneal:
           anneal = anneal_cap
        else:
             anneal = min(anneal_cap, update_count/total_anneal_steps)
        update_count += 1
        optimizer.zero_grad()
        X_out, mu, logvar = model(X_inp)
        loss = vae_loss_fn(X_inp, X_out, mu, logvar, anneal)
        train_step_2.anneal = anneal
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        avg_loss = running_loss / (batch_idx + 1)
        #t.set_postfix(loss=avg_loss)
        #return model

def eval_step_2(model,data_tr, data_te):
    model.eval()
    running_loss = 0.0
    eval_idxlist = list(range(data_tr.shape[0]))
    eval_N = data_tr.shape[0]
    eval_steps = len(range(0, eval_N, batch_size))
    n100_list = []
    with torch.no_grad():
         for batch_idx, start_idx in zip(range(eval_steps), range(0, eval_N, batch_size)):
             end_idx = min(start_idx + batch_size, eval_N)
             X_tr = data_tr[eval_idxlist[start_idx:end_idx]]
             X_te = data_te[eval_idxlist[start_idx:end_idx]]
             X_tr_inp = torch.FloatTensor(X_tr.toarray()).to(device)

             X_out, mu, logvar = model(X_tr_inp)
             loss = vae_loss_fn(X_tr_inp, X_out, mu, logvar, train_step_2.anneal)
             running_loss += loss.item()
             avg_loss = running_loss / (batch_idx + 1)

             #Exclude examples from training set
             X_out = X_out.cpu().numpy()
             X_out[X_tr.nonzero()] = -np.inf

             n100 = NDCG_binary_at_k_batch(X_out, X_te, k=100)
             n100_list.append(n100)
         n100_list = np.concatenate(n100_list)
         avg_NDCG = np.mean(n100_list)
    return avg_loss, avg_NDCG



"""## Anneal-EarlyStop-Init"""

def getTot_annealSTeps(train_RatMat,batch_size,n_epochs,anneal_cap):
    training_steps = len(range(0, train_RatMat.shape[0], batch_size))
    try:
       total_anneal_steps = (training_steps * (n_epochs - int(n_epochs * 0.2))) / anneal_cap  ##
    except ZeroDivisionError:
        assert (constant_anneal), "if 'anneal_cap' is set to 0.0 'constant_anneal' must be set to 'True"

    return total_anneal_steps

""" Early Stopping"""

def early_stop(curr_value, best_value, stop_step, patience, score_fn='metric'):
    if (score_fn == "loss" and curr_value <= best_value) or (score_fn == "metric" and curr_value >= best_value):
        stop_step, best_value = 0, curr_value
    else:
        stop_step += 1
    if stop_step >= patience:
        print("Early stopping triggered. patience: {} log:{}".format(patience, best_value))
        stop = True
    else:
        stop = False
    return best_value, stop_step, stop

"""Initialization"""

def init_weights(model):
    for name, param in model.named_parameters():
        if "weight" in name:
            nn.init.xavier_uniform_(param.data)
        elif "bias" in name:
            param.data.normal_(std=0.001)



"""## Tuning"""

def tuning_2(model,train_set,validtrain_set,validtest_set,als_param_grid,n_epochs,model_name,save_results= True):
    update_count = 0
    best_score = 0     ##based on MetricEval..
    stop_step = 0
    update_count = 0
    stop = False

    for i in tqdm(range(len(als_param_grid))):
        batch_size, lr_, weightdecay_ = list(als_param_grid)[i]
        total_anneal_steps = getTot_annealSTeps(train_set,batch_size,n_epochs,anneal_cap)
        Optimizer_ = torch.optim.AdamW(model.parameters(), lr=lr_, weight_decay=weightdecay_)
        print('batch_size = {} ,learning_rate = {}, weight_decay = {} '.format(batch_size,lr_,weightdecay_))
        model_2 = model
        for epoch in range(n_epochs):            
            train_step_2(model_2, Optimizer_, train_set,total_anneal_steps,epoch)
            val_loss, NDCG100_ = eval_step_2(model_2,validtrain_set,validtest_set)
            curr_score = NDCG100_
            best_score,stop_step,stop = early_stop(curr_score,best_score,stop_step,
                                           patience=25,score_fn='metric')
            if stop:
                break
            if (stop_step == 0) & (save_results):
               best_epoch = epoch
               best_BS  = batch_size
               best_lr_ = lr_
               best_weightdecay_ = weightdecay_
               best_NGCG100 = NDCG100_
               torch.save(model_2.state_dict(),model_name+".pth")
               #print('NDCG100_ = {} '.format(best_NGCG100))

        print('NDCG100_ = {} '.format(best_NGCG100))

    print('\n Best parameters; batch_size= {}, learning_rate = {}, weight_decay = {}, NDCG100_ = {}, best_epoch = {}'.format(best_BS,
                                                                            best_lr_,best_weightdecay_,best_NGCG100,best_epoch))
    return best_BS, best_lr_, best_weightdecay_,best_NGCG100, best_epoch

"""## get Predictions"""



def Trainwith_Best(model,UserItem_TRset,best_BS,best_lr,best_wd):
    total_anneal_steps = getTot_annealSTeps(UserItem_TRset,best_BS,anneal_n,anneal_cap)
    Optimizer_ = torch.optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)
    for epoch in tqdm(range(bst_epochs)):            
        train_step_2(model, Optimizer_, UserItem_TRset,total_anneal_steps,epoch)
      
    #torch.save(model.state_dict(),model_name+".pth")
    return model

def get_ModelOut(X_train,model):  #X_train== RatMat...
    with torch.no_grad():
         X_input = torch.FloatTensor(X_train.toarray()).to(device)
         X_out, mu, logvar = model(X_input)
         X_out = X_out.cpu().numpy()         #convert from tensor to numpy
         X_out[X_train.nonzero()] = -np.inf  #Exclude examples from training set
    return X_out


def getVAE_TopNPred(X_out,k):   ##X_out == tensor output from model
    batch_users = X_out.shape[0]
    idx_topk_part = bn.argpartition(-X_out, k, axis=1)
    topk_part = X_out[np.arange(batch_users)[:, np.newaxis], idx_topk_part[:, :k]]
    idx_part = np.argsort(-topk_part, axis=1)
    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]
    return idx_topk

def getVAE_AllPred(model,UserItemList,best_BS,best_lr,best_wd,k=10):
    ALLstep_Pred  = []
    for UserItem_ in UserItemList:
        Trainedmodel = Trainwith_Best(model,UserItem_,best_BS,best_lr,best_wd)
        X_out = get_ModelOut(UserItem_,Trainedmodel)
        topNPred = getVAE_TopNPred(X_out,k)   ##get prediction for all users..
        ALLstep_Pred.append(topNPred)
    return ALLstep_Pred


def VAEHitrate_Eval_2(allPred,holdout,user_column,item_column):  ##N == Top_N
    TestUsers = holdout[user_column]
    HOLDOUT_pred = allPred[TestUsers,:] 
    Eval_itemsVector  = holdout[[item_column]].to_numpy()
    HitRate_arr   =  (HOLDOUT_pred == Eval_itemsVector).sum(axis=1)  ##sum along row...
    HitCount = np.count_nonzero(HitRate_arr == 1)
    HitRate_ = HitRate_arr.mean()
    print("Number of hits: ", HitCount)
    print("Total Num of users: ",len(holdout[user_column]))
    print("Recommendation HitRate: ",HitRate_)
    return HitRate_

def VAEHitrate_Eval(allPred,holdout,user_column,item_column):  ##N == Top_N
    TestUsers = holdout[user_column]
    HOLDOUT_pred = allPred[TestUsers,:] 
    Eval_itemsVector  = holdout[[item_column]].to_numpy()
    HitRate_arr   =  (HOLDOUT_pred == Eval_itemsVector).sum(axis=1)  ##sum along row...
    HitCount = np.count_nonzero(HitRate_arr == 1)
    HitRate_ = HitRate_arr.mean()
    return HitRate_

def mean_confidence_interval(data, confidence=0.95):
    a = 1.0 * np.array(data)
    n = len(a)
    mean, se = np.mean(a), st.sem(a)
    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)
    lower_band =  mean-h
    upper_band =  mean+h
    return lower_band, mean,upper_band                ##LowerBand || Mean || UpperBand


def getAll_VAEHitRate(HOLDOUT_list,All_TOPN_PRED,user_column,item_column):
    AllSteps_Hitrate = []
    for holdout, allPred in zip(HOLDOUT_list,All_TOPN_PRED):                   
        HitRate_ = VAEHitrate_Eval_2(allPred,holdout,user_column,item_column)
        AllSteps_Hitrate.append(HitRate_)

    LowerBand, Avg_HitRate, UpperBand  = mean_confidence_interval(AllSteps_Hitrate, confidence=0.95)
    print("Average HitRate for All Recommendations: ", Avg_HitRate)
    return AllSteps_Hitrate, LowerBand, Avg_HitRate, UpperBand



